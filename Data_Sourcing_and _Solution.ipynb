{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\hp\\pycharmprojects\\test1\\venv\\lib\\site-packages (1.18.1)\n",
      "Wall time: 1.86 s\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\pycharmprojects\\test1\\venv\\lib\\site-packages (0.25.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\hp\\pycharmprojects\\test1\\venv\\lib\\site-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\hp\\pycharmprojects\\test1\\venv\\lib\\site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\hp\\pycharmprojects\\test1\\venv\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\pycharmprojects\\test1\\venv\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.13.0)\n",
      "Wall time: 1.4 s\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hp\\pycharmprojects\\test1\\venv\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hp\\pycharmprojects\\test1\\venv\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\hp\\pycharmprojects\\test1\\venv\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\hp\\pycharmprojects\\test1\\venv\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\pycharmprojects\\test1\\venv\\lib\\site-packages (from matplotlib) (0.10.0)Wall time: 1.37 s\n",
      "\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\hp\\pycharmprojects\\test1\\venv\\lib\\site-packages (from matplotlib) (1.18.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\pycharmprojects\\test1\\venv\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.13.0)\n",
      "Requirement already satisfied: sklearn in c:\\users\\hp\\pycharmprojects\\test1\\venv\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\pycharmprojects\\test1\\venv\\lib\\site-packages (from sklearn) (0.23.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\pycharmprojects\\test1\\venv\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\hp\\pycharmprojects\\test1\\venv\\lib\\site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\hp\\pycharmprojects\\test1\\venv\\lib\\site-packages (from scikit-learn->sklearn) (0.16.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\hp\\pycharmprojects\\test1\\venv\\lib\\site-packages (from scikit-learn->sklearn) (1.18.1)\n",
      "Wall time: 1.44 s\n",
      "Requirement already satisfied: argparse in c:\\users\\hp\\pycharmprojects\\test1\\venv\\lib\\site-packages (1.4.0)\n",
      "Wall time: 1.34 s\n"
     ]
    }
   ],
   "source": [
    "# install packages\n",
    "import sys\n",
    "%time  !{sys.executable} -m pip install numpy\n",
    "%time  !{sys.executable} -m pip install pandas\n",
    "%time  !{sys.executable} -m pip install matplotlib\n",
    "%time  !{sys.executable} -m pip install sklearn\n",
    "%time  !{sys.executable} -m pip install datetime\n",
    "%time  !{sys.executable} -m pip install argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\hp\\pycharmprojects\\test1\\venv\\lib\\site-packages (1.0.1)\n",
      "Wall time: 1.35 s\n"
     ]
    }
   ],
   "source": [
    "# method 1 to install 'torch'\n",
    "%time  !{sys.executable} -m pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2 to install 'torch'\n",
    "!pip3 install https://download.pytorch.org/whl/cpu/torch-1.0.1-cp37-cp37m-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read database about the confirmed cases and try to filter out the area\n",
    "import pandas as pd\n",
    "import datetime\n",
    "url=\"https://raw.githubusercontent.com/datadesk/california-coronavirus-data/master/latimes-place-totals.csv\"\n",
    "df = pd.read_csv(url,header=0)\n",
    "df = df[df['county']=='Los Angeles'][['date','place','confirmed_cases']]\n",
    "\n",
    "target = ['Alhambra', 'Arcadia', 'Beverly Hills', 'Boyle Heights', 'Carson', 'Diamond Bar', 'Encino', 'Gardena', 'Glendale', 'Glendora',\n",
    "          'Granada Hills', 'Inglewood', 'La Mirada', 'Lancaster', 'Manhattan Beach', 'Melrose', 'Northridge', 'San Dimas', 'San Pedro',\n",
    "          'Santa Monica', 'Sherman Oaks', 'Silver Lake', 'Tarzana', 'Torrance', 'Venice', 'West Adams', 'West Hills', 'West Hollywood',\n",
    "          'West Vernon', 'Westchester', 'Altadena', 'Baldwin Hills', 'Brentwood', 'Culver City', 'Eagle Rock', 'Hollywood',\n",
    "          'Hollywood Hills', 'Lynwood', 'Mar Vista', 'Monterey Park', 'North Hollywood', 'Reseda', 'Santa Clarita', 'Woodland Hills',\n",
    "          'Sylmar', 'Walnut', 'Beverlywood', 'Burbank', 'Calabasas', 'Castaic', 'Covina', 'Crestview', 'East Los Angeles', 'Echo Park', \n",
    "          'Hancock Park', 'Hawthorne', 'Lawndale', 'Lomita', 'Palms', 'Playa Vista', 'South El Monte', 'Stevenson Ranch', 'Studio City',\n",
    "          'Tujunga', 'University Park', 'Valley Glen', 'Van Nuys', 'Vermont Knolls', 'Westwood', 'Whittier', 'Century City', 'El Segundo',\n",
    "          'Lake Balboa', 'Lakewood', 'Miracle Mile', 'Park La Brea', 'Redondo Beach', 'San Fernando', 'South Whittier', 'Winnetka', \n",
    "          'Del Rey', 'La Canada Flintridge', 'La Verne', 'Montebello', 'Sun Valley', 'Sunland', 'Vermont Vista', 'Vernon Central',\n",
    "          'West Covina', 'Westlake', 'Bellflower', 'Canoga Park', 'East Hollywood', 'Los Feliz', 'Paramount', 'Rancho Palos Verdes', \n",
    "          'South Gate', 'Agoura Hills', 'Duarte', 'Exposition Park', 'Hyde Park', 'Lincoln Heights', 'Palmdale', 'South Park',\n",
    "          'Wilshire Center', 'Canyon Country', 'Claremont', 'Downey', 'Harbor Gateway', 'Harvard Heights', 'Highland Park', \n",
    "          'La Puente', 'Norwalk', 'Pico Rivera', 'Porter Ranch', 'San Gabriel', 'Wholesale District', 'Willowbrook', 'Arleta',\n",
    "          'Bell Gardens', 'Glassell Park', 'Panorama City', 'Pomona', 'Valinda', 'Watts', 'Azusa', 'Bell', 'Chatsworth', \n",
    "          'Hacienda Heights', 'Harbor City', 'Leimert Park', 'Maywood', 'Monrovia', 'North Hills', 'Pacoima', 'Avalon', 'Baldwin Park',\n",
    "          'Bassett', 'Central', 'El Monte', 'El Sereno', 'Harvard Park', 'Lake Los Angeles', 'Rosemead', 'Rowland Heights', 'Temple City',\n",
    "          'Acton', 'Cerritos', 'Cloverdale/Cochran', 'Compton', 'Downtown', 'Huntington Park', 'Koreatown', 'Mt. Washington', 'Pasadena', \n",
    "          'South Pasadena', 'Wilmington']\n",
    "\n",
    "df = df[df['place'].isin(target)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Data Type \n",
    "result = df.copy()\n",
    "result['place'].replace('Silver Lake','Silverlake',inplace = True)\n",
    "result['date'] = pd.to_datetime(result['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Add New Columns\n",
    "result = result.sort_values(by=['place','date'])\n",
    "result['new_confirmed_cases'] = result['confirmed_cases']\n",
    "result['ave_new7_10after'] = result['confirmed_cases']\n",
    "result['ave_new6_9after'] = result['confirmed_cases']\n",
    "result['ave_new8_11after'] = result['confirmed_cases']\n",
    "result.iloc[0,3] = None\n",
    "\n",
    "# Calculate Daily New Cases\n",
    "for i in range(1,len(result)):\n",
    "    if result.iloc[i,1] != result.iloc[(i-1),1]:\n",
    "        result.iloc[i,3] = None\n",
    "    else:\n",
    "        result.iloc[i,3] = max([0,int(result.iloc[i,2])-int(result.iloc[(i-1),2])])    \n",
    "# Calculate Future Cases in AVG\n",
    "for i in range(1,len(result)):    \n",
    "    if i < (len(result)-11):\n",
    "        if result.iloc[i,1] == result.iloc[(i+11),1]:\n",
    "            result.iloc[i,4] = (result.iloc[(i+7),3] + result.iloc[(i+8),3] + result.iloc[(i+9),3] + result.iloc[(i+10),3])/4.0\n",
    "            result.iloc[i,5] = (result.iloc[(i+6),3] + result.iloc[(i+7),3] + result.iloc[(i+8),3] + result.iloc[(i+9),3])/4.0\n",
    "            result.iloc[i,6] = (result.iloc[(i+8),3] + result.iloc[(i+9),3] + result.iloc[(i+10),3] + result.iloc[(i+11),3])/4.0\n",
    "        elif result.iloc[i,1] == result.iloc[(i+10),1]:\n",
    "            result.iloc[i,4] = (result.iloc[(i+7),3] + result.iloc[(i+8),3] + result.iloc[(i+9),3] + result.iloc[(i+10),3])/4.0\n",
    "            result.iloc[i,5] = (result.iloc[(i+6),3] + result.iloc[(i+7),3] + result.iloc[(i+8),3] + result.iloc[(i+9),3])/4.0\n",
    "            result.iloc[i,6] = (result.iloc[(i+8),3] + result.iloc[(i+9),3] + result.iloc[(i+10),3])/3.0\n",
    "        elif result.iloc[i,1] == result.iloc[(i+9),1]:\n",
    "            result.iloc[i,4] = (result.iloc[(i+7),3] + result.iloc[(i+8),3] + result.iloc[(i+9),3])/3.0\n",
    "            result.iloc[i,5] = (result.iloc[(i+6),3] + result.iloc[(i+7),3] + result.iloc[(i+8),3] + result.iloc[(i+9),3])/4.0\n",
    "            result.iloc[i,6] = (result.iloc[(i+8),3] + result.iloc[(i+9),3])/2.0\n",
    "        elif result.iloc[i,1] == result.iloc[(i+8),1]:\n",
    "            result.iloc[i,4] = (result.iloc[(i+7),3] + result.iloc[(i+8),3])/2.0\n",
    "            result.iloc[i,5] = (result.iloc[(i+6),3] + result.iloc[(i+7),3] + result.iloc[(i+8),3])/3.0\n",
    "            result.iloc[i,6] = result.iloc[(i+8),3]\n",
    "        elif result.iloc[i,1] == result.iloc[(i+7),1]:\n",
    "            result.iloc[i,5] = (result.iloc[(i+6),3] + result.iloc[(i+7),3])/2.0\n",
    "            result.iloc[i,6] = result.iloc[i,4] = result.iloc[(i+7),3]\n",
    "        else:\n",
    "            for j in range(8):\n",
    "                if result.iloc[i,1] == result.iloc[(i+7-j),1]:\n",
    "                    result.iloc[i,4] = result.iloc[i,5] = result.iloc[i,6] = result.iloc[(i+7-j),3]\n",
    "                    break\n",
    "    else:\n",
    "        if i < (len(result)-10):\n",
    "            result.iloc[i,4] = (result.iloc[(i+7),3] + result.iloc[(i+8),3] + result.iloc[(i+9),3] + result.iloc[(i+10),3])/4.0\n",
    "            result.iloc[i,5] = (result.iloc[(i+6),3] + result.iloc[(i+7),3] + result.iloc[(i+8),3] + result.iloc[(i+9),3])/4.0\n",
    "            result.iloc[i,6] = (result.iloc[(i+8),3] + result.iloc[(i+9),3] + result.iloc[(i+10),3])/3.0\n",
    "        elif i < (len(result)-9):\n",
    "            result.iloc[i,4] = (result.iloc[(i+7),3] + result.iloc[(i+8),3] + result.iloc[(i+9),3])/3.0\n",
    "            result.iloc[i,5] = (result.iloc[(i+6),3] + result.iloc[(i+7),3] + result.iloc[(i+8),3] + result.iloc[(i+9),3])/4.0\n",
    "            result.iloc[i,6] = (result.iloc[(i+8),3] + result.iloc[(i+9),3])/2.0\n",
    "        elif i < (len(result)-8):\n",
    "            result.iloc[i,4] = (result.iloc[(i+7),3] + result.iloc[(i+8),3])/2.0\n",
    "            result.iloc[i,5] = (result.iloc[(i+6),3] + result.iloc[(i+7),3] + result.iloc[(i+8),3])/3.0\n",
    "            result.iloc[i,6] = result.iloc[(i+8),3]\n",
    "        elif i < (len(result)-7):\n",
    "            result.iloc[i,5] = (result.iloc[(i+6),3] + result.iloc[(i+7),3])/2.0\n",
    "            result.iloc[i,6] = result.iloc[i,4] = result.iloc[(i+7),3]\n",
    "        else:          \n",
    "            result.iloc[i,4] = result.iloc[i,5] = result.iloc[i,6] = result.iloc[-1,3]\n",
    "\n",
    "result.dropna(inplace = True)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [WinError 10061] 由于目标计算机积极拒绝，无法连接。>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mdo_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[1;32m-> 1317\u001b[1;33m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0m\u001b[0;32m   1318\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# timeout error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\http\\client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1243\u001b[0m         \u001b[1;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1244\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1289\u001b[0m             \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'body'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1290\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\http\\client.py\u001b[0m in \u001b[0;36mendheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1238\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1025\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\http\\client.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    965\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 966\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\http\\client.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1406\u001b[1;33m             \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\http\\client.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    937\u001b[0m         self.sock = self._create_connection(\n\u001b[1;32m--> 938\u001b[1;33m             (self.host,self.port), self.timeout, self.source_address)\n\u001b[0m\u001b[0;32m    939\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetsockopt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIPPROTO_TCP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTCP_NODELAY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    726\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    715\u001b[0m                 \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# Break explicitly a reference cycle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] 由于目标计算机积极拒绝，无法连接。",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7334d946ab2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0murl2\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m\"https://covid19-static.cdn-apple.com/covid19-mobility-data/2013HotfixDev8/v3/en-us/applemobilitytrends-2020-07-25.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mapple_mobility\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0murl3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://www.gstatic.com/covid19/mobility/Global_Mobility_Report.csv?cachebust=04188f017409e90a\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\pycharmprojects\\test1\\venv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\pycharmprojects\\test1\\venv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[1;31m# See https://github.com/python/mypy/issues/1297\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     fp_or_buf, _, compression, should_close = get_filepath_or_buffer(\n\u001b[1;32m--> 440\u001b[1;33m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m     )\n\u001b[0;32m    442\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\pycharmprojects\\test1\\venv\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m         \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m         \u001b[0mcontent_encoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Content-Encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcontent_encoding\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"gzip\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;31m# post-process response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[1;32m--> 543\u001b[1;33m                                   '_open', req)\n\u001b[0m\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1358\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1359\u001b[0m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[1;32m-> 1360\u001b[1;33m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[0m\u001b[0;32m   1361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1362\u001b[0m         \u001b[0mhttps_request\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mdo_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1317\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0;32m   1318\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# timeout error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1319\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1320\u001b[0m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mURLError\u001b[0m: <urlopen error [WinError 10061] 由于目标计算机积极拒绝，无法连接。>"
     ]
    }
   ],
   "source": [
    "# Read Mobility Data\n",
    "\n",
    "url2= \"https://covid19-static.cdn-apple.com/covid19-mobility-data/2013HotfixDev8/v3/en-us/applemobilitytrends-2020-07-25.csv\"\n",
    "apple_mobility = pd.read_csv(url2,header=0)\n",
    "\n",
    "url3 = \"https://www.gstatic.com/covid19/mobility/Global_Mobility_Report.csv?cachebust=04188f017409e90a\"\n",
    "google_mobility = pd.read_csv(url3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit Mobility Data for Merging\n",
    "apple_mobility = apple_mobility[apple_mobility['region']=='Los Angeles']\n",
    "google_mobility = google_mobility[google_mobility['sub_region_2'] == 'Los Angeles County']\n",
    "\n",
    "amobility= apple_mobility.copy()\n",
    "amobility = amobility.set_index('transportation_type').transpose().iloc[5:]\n",
    "\n",
    "amobility['date'] = pd.to_datetime(amobility.index)\n",
    "\n",
    "gmobility = google_mobility[['date','retail_and_recreation_percent_change_from_baseline',\n",
    "                       'grocery_and_pharmacy_percent_change_from_baseline',\n",
    "                       'parks_percent_change_from_baseline',\n",
    "                       'transit_stations_percent_change_from_baseline',\n",
    "                       'workplaces_percent_change_from_baseline',\n",
    "                       'residential_percent_change_from_baseline',]].copy()\n",
    "gmobility['date'] = pd.to_datetime(google_mobility['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and Modify Econ Data\n",
    "url4 = \"https://raw.githubusercontent.com/skasralikar/Risk-Score-1-UMichZJU/master/econ_level.csv\"\n",
    "econ = pd.read_csv(url4, index_col = 0)\n",
    "econ.columns = ['place','Density_Per_Sq_Mile','population','Income_level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Mobility Data\n",
    "final = result.merge(amobility,how = 'left',on = 'date')\n",
    "final_result = final.merge(gmobility,how = 'left',on = 'date')\n",
    "final_result.dropna(inplace = True)\n",
    "\n",
    "# Merge Econ Data\n",
    "final_results = final_result.merge(econ, how = 'left', on = 'place')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results['ZIP'] = final_results['place']\n",
    "final_results['ZIP'].astype(str)\n",
    "final_results['date'].astype(str)\n",
    "\n",
    "LA_daily = final_results[['ZIP','date','confirmed_cases','new_confirmed_cases','population','Density_Per_Sq_Mile','Income_level','driving','transit',\n",
    "                          'walking','retail_and_recreation_percent_change_from_baseline',\n",
    "                          'grocery_and_pharmacy_percent_change_from_baseline',\n",
    "                          'parks_percent_change_from_baseline','transit_stations_percent_change_from_baseline',\n",
    "                          'workplaces_percent_change_from_baseline','residential_percent_change_from_baseline',\n",
    "                          'ave_new7_10after','ave_new6_9after','ave_new8_11after']]\n",
    "\n",
    "LA_daily_predict = final_results[['ZIP','date','confirmed_cases','new_confirmed_cases','population','Density_Per_Sq_Mile','Income_level','driving',\n",
    "                                  'transit','walking','retail_and_recreation_percent_change_from_baseline',\n",
    "                                  'grocery_and_pharmacy_percent_change_from_baseline',\n",
    "                                  'parks_percent_change_from_baseline','transit_stations_percent_change_from_baseline',\n",
    "                                  'workplaces_percent_change_from_baseline','residential_percent_change_from_baseline',\n",
    "                                  'ave_new7_10after']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(LA_daily.head(10))\n",
    "print(LA_daily_predict.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter_daily.py\n",
    "\"\"\"    super parameter     \"\"\"\n",
    "day_input = 6                      # how many days' feature we use to predict\n",
    "timelagging = 6                     # the length of latent window of Social Distancing data & Mobility data\n",
    "average_num = 4                     # how many days does the average cases get from\n",
    "### change the feature numbers  ###\n",
    "feature_num = 14+2*(day_input-1)    # the number of feature that one input contains\n",
    "### -------------------------  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_daily.py\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from parameter_daily import day_input, average_num, feature_num, timelagging\n",
    "\n",
    "class Mydataset(Dataset):\n",
    "    def __init__(self, input, output):\n",
    "        super(Mydataset, self).__init__()\n",
    "        self.input = input\n",
    "        self.lable = output\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.input[index], self.lable[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "\n",
    "def dataset_generate_daily():\n",
    "    \"\"\"    read data in and clean!     \"\"\"\n",
    "\n",
    "    ### change the input datset name ###\n",
    "#     filename = \"LA_daily.csv\"\n",
    "    ### ---------------------------- ###\n",
    "    zipcode_daily = LA_daily\n",
    "#     pd.read_csv(filename, encoding=\"ISO-8859-1\", dtype={'ZIP': str, 'date': str})\n",
    "    zip = zipcode_daily['ZIP']  # 'ZIP' column\n",
    "    date = zipcode_daily['date']  # we have to preserve the date\n",
    "    del zipcode_daily['ZIP']  # delete the non-numeric columns\n",
    "    del zipcode_daily['date']\n",
    "    zipcode_daily = pd.DataFrame(zipcode_daily, dtype=float)  # change the type from 'int' to 'float'\n",
    "    zipcode_daily['ZIP'] = zip  # add the 'ZIP' column again\n",
    "\n",
    "    # key: 'zip code', value: feature that belong to the 'zip code'\n",
    "    data_dict = {}\n",
    "    for i, zipcode in enumerate(zipcode_daily[:]['ZIP']):\n",
    "        if zipcode not in data_dict:\n",
    "            data_dict[zipcode] = []\n",
    "        feature = []\n",
    "        for f in zipcode_daily.iloc[i]:\n",
    "            feature.append(f)\n",
    "        data_dict[zipcode].append(feature)\n",
    "\n",
    "    data_x = []  # input\n",
    "    data_y = []  # lable\n",
    "    for key, values in data_dict.items():\n",
    "        l = len(values)\n",
    "        input_num = l - timelagging - average_num  # determine how many input here\n",
    "        feature = []\n",
    "        for i in range(input_num):  # one input point contains 6 days's data, that is day1~day6\n",
    "            first = True\n",
    "            for j in range(day_input):\n",
    "                if first:  # one input point contains all the feature of day1\n",
    "                    for k in values[i][:-4]:\n",
    "                        feature.append(k)\n",
    "                    first = False\n",
    "                else:  # for day2~day6, one input point only contains confirmed_cases & new_confirmed_cases\n",
    "                    feature.append(values[i + j][0])\n",
    "                    feature.append(values[i + j][1])\n",
    "            data_y.append(values[i][-4])  # output: average cases, that is ave_new7_10after\n",
    "            tmp = []\n",
    "            tmp.append(feature)\n",
    "            data_x.append(tmp)  # size: [1, feature_num]\n",
    "            feature = []\n",
    "    # split data to train and test, and split test to validation and test in the following.\n",
    "    train_x, test_x, train_y, test_y = model_selection.train_test_split(data_x, data_y, test_size=0.3,\n",
    "                                                                        random_state=1)\n",
    "\n",
    "    train_x_ls = []  # Change the format for later processing\n",
    "    for j in train_x:\n",
    "        for i in j:\n",
    "            train_x_ls.append(i)\n",
    "    train_x_df = pd.DataFrame(train_x_ls)\n",
    "    train_y_df = pd.DataFrame(train_y)\n",
    "    train_x_mean = train_x_df.mean()  # train_x dataset mean\n",
    "    train_x_std = train_x_df.std()  # train_x dataset std\n",
    "    train_y_mean = train_y_df.mean()  # train_y dataset mean\n",
    "    train_y_std = train_y_df.std()  # train_y dataset std\n",
    "\n",
    "    for i in range(len(train_x)):       # using train_x mean and train_x std to normalize\n",
    "        for j in range(len(train_x[i])):\n",
    "            for k in range(len(train_x[i][j])):\n",
    "                train_x[i][j][k] = (train_x[i][j][k] - train_x_mean[k]) / train_x_std[k]\n",
    "\n",
    "    for i in range(len(train_y)):       # using train_y mean and train_y std to normalize\n",
    "        train_y[i] = (train_y[i] - train_y_mean) / train_y_std\n",
    "\n",
    "    for i in range(len(test_x)):        # using train_x mean and train_x std to normalize\n",
    "        for j in range(len(test_x[i])):\n",
    "            for k in range(len(test_x[i][j])):\n",
    "                test_x[i][j][k] = (test_x[i][j][k] - train_x_mean[k]) / train_x_std[k]\n",
    "\n",
    "    for i in range(len(test_y)):        # using train_y mean and train_y std to normalize\n",
    "        test_y[i] = (test_y[i] - train_y_mean) / train_y_std\n",
    "\n",
    "    # split test to validation and test\n",
    "    validation_x, test_x, validation_y, test_y = model_selection.train_test_split(test_x, test_y, test_size=0.5,\n",
    "                                                                                  random_state=1)\n",
    "    train_x = torch.tensor(train_x)\n",
    "    train_y = torch.tensor(train_y).reshape(-1, 1)\n",
    "    validation_x = torch.tensor(validation_x)\n",
    "    validation_y = torch.tensor(validation_y).reshape(-1, 1)\n",
    "    test_x = torch.tensor(test_x)\n",
    "    test_y = torch.tensor(test_y).reshape(-1, 1)\n",
    "\n",
    "    # define dataset\n",
    "    train_data = Mydataset(train_x, train_y)\n",
    "    trainloader = DataLoader(train_data, batch_size=256, shuffle=True)\n",
    "\n",
    "    return trainloader, train_x, train_y, validation_x, validation_y, \\\n",
    "           test_x, test_y, train_x_mean, train_x_std, train_y_mean, train_y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function.py\n",
    "import torch\n",
    "\n",
    "\n",
    "# scale the output value back to its original size and cal the loss\n",
    "def loss_cal(predict, lable, train_mean, train_std):\n",
    "    x = predict[:]\n",
    "    y = lable[:]\n",
    "    for i in range(len(predict)):\n",
    "        x[i][0] = x[i][0] * torch.tensor(train_std) + torch.tensor(train_mean)\n",
    "        y[i][0] = y[i][0] * torch.tensor(train_std) + torch.tensor(train_mean)\n",
    "    loss_fun = torch.nn.MSELoss()\n",
    "    loss = loss_fun(x, y)\n",
    "    return loss, x, y\n",
    "\n",
    "\n",
    "# change the learning rate\n",
    "def adjust_learning_rate(optimizer, learning_r=None):\n",
    "    i = 0\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = learning_r\n",
    "        if i == 0:\n",
    "            print(\"optimizer lr : {}\".format(param_group['lr']))\n",
    "            i += 1\n",
    "\n",
    "\n",
    "\"\"\"     define LSTM model   \"\"\"\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, feature_num):\n",
    "        super(Net, self).__init__()\n",
    "        # if batch_first=True, then input shape = (batch, seq, shape)\n",
    "        self.lstm = torch.nn.LSTM(input_size=feature_num, hidden_size=64, num_layers=1, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(64 * 1, 32)\n",
    "        self.linear1 = torch.nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.reshape(-1, 64 * 1)\n",
    "        x = self.linear(x)\n",
    "        x = self.linear1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_daily.py\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#from dataset_daily import dataset_generate_daily\n",
    "#from function import loss_cal, adjust_learning_rate, Net\n",
    "#from parameter_daily import feature_num\n",
    "\n",
    "\n",
    "def train_daily():\n",
    "\n",
    "    trainloader, train_x, train_y, validation_x, validation_y, test_x, test_y, \\\n",
    "    train_x_mean, train_x_std, train_y_mean, train_y_std = dataset_generate_daily()\n",
    "\n",
    "    \"\"\"         training start!         \"\"\"\n",
    "    # determine optimizer, loss_function and checkpoint path\n",
    "    model = Net(feature_num)\n",
    "    learning_r = 0.0002\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_r)\n",
    "    loss_fun = torch.nn.MSELoss()\n",
    "    ### change the path name ###\n",
    "    path = 'checkpoint_0727.tar'\n",
    "    ### ------------------- ###\n",
    "\n",
    "    \"\"\" if you want train your model from a pre-trained model, uncomment the following code. \"\"\"\n",
    "    # checkpoint = torch.load(path)\n",
    "    # model.load_state_dict(checkpoint['net'])\n",
    "    # optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    # validation_loss = checkpoint['best_validation_loss']\n",
    "    # learning_r = 6.400000000000004e-8\n",
    "\n",
    "    model.train()\n",
    "    # val_loss_info = [validation_loss]\n",
    "    val_loss_info = []      # preserve every epoch's train loss\n",
    "    train_loss_info = []    # preserve every epoch's validation loss\n",
    "    not_improve = 0\n",
    "    for epoch in range(1500):\n",
    "        for i, values in enumerate(trainloader):\n",
    "            input, lable = values\n",
    "            output = model(input)\n",
    "            loss = loss_fun(output, lable)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if epoch % 10 == 0 and epoch > 0:\n",
    "            train_loss = loss_fun(model(train_x),train_y).item()\n",
    "            vali_loss = loss_fun(model(validation_x), validation_y).item()\n",
    "            if len(val_loss_info) == 0 or vali_loss < min(val_loss_info):  # save the best model\n",
    "                not_improve = 0\n",
    "                state = {'net': model.state_dict(), 'optimizer': optimizer.state_dict(),'best_validation_loss': vali_loss}\n",
    "                torch.save(state, path)\n",
    "                print(\"Saved the model.\")\n",
    "            else:                           # if validation loss didn't decrease, reload the best model in next epoch.\n",
    "                checkpoint = torch.load(path)\n",
    "                model.load_state_dict(checkpoint['net'])\n",
    "                optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "                not_improve += 1\n",
    "                print(\"not_improve : {}\".format(not_improve))\n",
    "            if (not_improve+1) % 8 == 0:    # when validation loss doesn't decrease for 7 epochs, reduce the learning rate.\n",
    "                learning_r *= 0.2\n",
    "                adjust_learning_rate(optimizer, learning_r)\n",
    "            if (not_improve+1) % 25 == 0:   # early stopping\n",
    "                print(\"Training End......\")\n",
    "                break\n",
    "\n",
    "            print(\"epoch:{}, train_loss:{}, vali_loss: {}\".format(epoch, train_loss, vali_loss))\n",
    "            train_loss_info.append(train_loss)\n",
    "            val_loss_info.append(vali_loss)\n",
    "\n",
    "    return train_loss_info, val_loss_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_daily.py\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "#from function import loss_cal, adjust_learning_rate, Net\n",
    "#from parameter_daily import feature_num\n",
    "import numpy as np\n",
    "\n",
    "def test_daily(path, test_x, test_y, train_mean, train_std):\n",
    "    \"\"\"     test start!     \"\"\"\n",
    "    # using the test dataset to test model\n",
    "    model = Net(feature_num)\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['net'])\n",
    "    model.eval()\n",
    "    predict = model(test_x)\n",
    "    real = test_y.clone()\n",
    "\n",
    "    loss_fun = torch.nn.MSELoss()\n",
    "    # two losses, one is normalized scale, another is original scale\n",
    "    loss = loss_fun(predict, real)\n",
    "    loss_original_scale, pre, rea = loss_cal(predict, real, train_mean, train_std)\n",
    "    pre = np.array(pre.data)\n",
    "    rea = np.array(rea.data)\n",
    "    print(\"Test loss: \" + str(loss))\n",
    "    print(\"Test loss in original scale: \" + str(loss_original_scale))\n",
    "\n",
    "    # true output and predicted output\n",
    "    plt.figure(2)\n",
    "    plt.plot(list(rea), label=\"real\")\n",
    "    plt.plot(list(pre), label=\"pred\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    # true output and predicted output\n",
    "    plt.figure(3)\n",
    "    min_val = min(rea)\n",
    "    max_val = max(rea)\n",
    "    plt.scatter(rea,pre)\n",
    "    plt.plot([min_val,max_val],[min_val,max_val],color = 'red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "\"\"\"\n",
    "\n",
    "In this project, We created a LSTM model to predict covid-19 daily new_cases and weekly new cases in Los Angeles.\n",
    "\n",
    "If you find anything wrong with the code, please feel free to contact us.\n",
    "\n",
    "@University: Umich & ZJU\n",
    "@author: Wenxue Li, Zixian Ma, Xinyu Li\n",
    "@email: liwenxue@zju.edu.cn, 3170103467@zju.edu.cn\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "from dataset_daily import dataset_generate_daily\n",
    "from dataset_weekly import dataset_generate\n",
    "from train_daily import train_daily\n",
    "from train_weekly import train_weekly\n",
    "from test_daily import test_daily\n",
    "from test_weekly import test_weekly\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='COVID-LSTM')\n",
    "parser.add_argument('--mode', default='train', type=str, help='choose train or test')\n",
    "parser.add_argument('--type', default='daily', type=str, help='daily or weekly')\n",
    "\n",
    "args = parser.parse_args(args=[]) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.mode == 'train':\n",
    "    if args.type == 'daily':\n",
    "        train_loss_info, val_loss_info = train_daily()\n",
    "        # training loss trend and validation loss trend\n",
    "        plt.figure(1)\n",
    "        plt.plot(train_loss_info, label='train_loss')\n",
    "        plt.plot(val_loss_info, label='val_loss')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "    if args.type == 'weekly':\n",
    "        train_loss_info, val_loss_info = train_weekly()\n",
    "        # training loss trend and validation loss trend\n",
    "        plt.figure(1)\n",
    "        plt.plot(train_loss_info, label='train_loss')\n",
    "        plt.plot(val_loss_info, label='val_loss')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.mode == 'test':\n",
    "    if args.type == 'daily':\n",
    "        trainloader, train_x, train_y, validation_x, validation_y, \\\n",
    "        test_x, test_y, train_x_mean, train_x_std, train_y_mean, train_y_std = dataset_generate_daily()\n",
    "        # path = 'checkpoint_1.tar'\n",
    "        path = 'checkpoint_daily.tar'\n",
    "        test_daily(path, test_x, test_y, train_y_mean, train_y_std)\n",
    "    if args.type == 'weekly':\n",
    "        # path = 'checkpoint_2.tar'\n",
    "        path = 'checkpoint_weekly_sequential.tar'\n",
    "        trainloader, train_x, train_y, validation_x, validation_y, \\\n",
    "        test_x, test_y, train_x_mean, train_x_std, train_y_mean, train_y_std = dataset_generate()\n",
    "        test_weekly(path, test_x, test_y, train_y_mean, train_y_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily_predict_4_day_avg.py\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "day_input = 6\n",
    "timelagging = 6\n",
    "average_num = 4\n",
    "### change the numbers of feature ###\n",
    "feature_num = 14+2*(day_input-1)\n",
    "### ------------------------###\n",
    "\n",
    "\"\"\"     read in data and process, this part is very similar to lstm_1.py    \"\"\"\n",
    "### change the input dataset name ###\n",
    "# filename = \"LA_daily_predict.csv\"\n",
    "### --------------------------- ###\n",
    "zipcode_daily = LA_daily_predict\n",
    "# pd.read_csv(filename, encoding=\"ISO-8859-1\", dtype={'ZIP': str, 'date': str})\n",
    "zip = zipcode_daily['ZIP']\n",
    "date = zipcode_daily['date']  # we have to preserve the date\n",
    "del zipcode_daily['ZIP']\n",
    "del zipcode_daily['date']\n",
    "zipcode_daily = pd.DataFrame(zipcode_daily, dtype=float)  # change the type from 'int' to 'float'\n",
    "zipcode_daily['ZIP'] = zip\n",
    "zipcode_daily['date'] = date  # add date back\n",
    "\n",
    "data_dict = {}  # key: 'zip', value: feature that belong to the key\n",
    "for i, zipcode in enumerate(zipcode_daily[:]['ZIP']):\n",
    "    if zipcode not in data_dict:\n",
    "        data_dict[zipcode] = []\n",
    "    feature = []\n",
    "    for f in zipcode_daily.iloc[i]:\n",
    "        feature.append(f)\n",
    "    data_dict[zipcode].append(feature)\n",
    "\n",
    "zipcode = []  # save 'zip code' correlating to input point\n",
    "date = []     # save 'date' correlating to input point\n",
    "\n",
    "data_x = []  # input\n",
    "for key, values in data_dict.items():\n",
    "    l = len(values)\n",
    "    input_num = l - timelagging - average_num\n",
    "    feature = []\n",
    "    input_num=input_num+5\n",
    "    for i in range(input_num):\n",
    "        first = True\n",
    "        for j in range(day_input):\n",
    "            if first:\n",
    "                zipcode.append(values[i][-2])   # save 'zip code' correlating to ont input point\n",
    "                date.append(values[i][-1])      # save 'date' correlating to ont input point\n",
    "                # because we add 'date' back, the last feature is values[i][:-5] not values[i][:-4]\n",
    "                for k in values[i][:-3]:\n",
    "                    feature.append(k)\n",
    "                first = False\n",
    "            else:\n",
    "                feature.append(values[i + j][0])\n",
    "                feature.append(values[i + j][1])\n",
    "        tmp = []\n",
    "        tmp.append(feature)\n",
    "        data_x.append(tmp)  # one input point\n",
    "        feature = []\n",
    "\n",
    "data_x_ls = []\n",
    "for j in data_x:\n",
    "    for i in j:\n",
    "        data_x_ls.append(i)\n",
    "data_x_df = pd.DataFrame(data_x_ls)\n",
    "data_x_mean = data_x_df.mean()  # train dataset mean\n",
    "data_x_std = data_x_df.std()    # train dataset std\n",
    "\n",
    "for i in range(len(data_x)):    # using train_x mean and train_x std to normalize\n",
    "    for j in range(len(data_x[i])):\n",
    "        for k in range(len(data_x[i][j])):\n",
    "            data_x[i][j][k] = (data_x[i][j][k] - data_x_mean[k]) / data_x_std[k]\n",
    "\n",
    "data_x = torch.tensor(data_x)\n",
    "\n",
    "\n",
    "# scale the output value back to its original size and cal the loss\n",
    "def upscale(predict):\n",
    "    x = predict[:]\n",
    "    for i in range(len(predict)):\n",
    "        x[i][0] = x[i][0] * torch.tensor(data_x_std[1]) + torch.tensor(data_x_mean[1])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"     define LSTM model   \"\"\"\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # if batch_first=True, then input shape = (batch, seq, shape)\n",
    "        self.lstm = torch.nn.LSTM(input_size=feature_num, hidden_size=64, num_layers=1, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(64 * 1, 32)\n",
    "        self.linear1 = torch.nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.reshape(-1, 64 * 1)\n",
    "        x = self.linear(x)\n",
    "        x = self.linear1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net()\n",
    "### change the path name ###\n",
    "path = 'checkpoint_0727.tar'\n",
    "### -------------------- ###\n",
    "# load from file\n",
    "checkpoint = torch.load(path)\n",
    "model.load_state_dict(checkpoint['net'])\n",
    "model.eval()\n",
    "predict = np.array(model(data_x).data)  # output\n",
    "predict_upscale = upscale(predict)      # original scale output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "###\n",
    "date_list = []\n",
    "for i in range(1,31):\n",
    "    date_list.append('2020-04-'+str(i))\n",
    "for i in range(1,32):\n",
    "    date_list.append('2020-05-'+str(i))\n",
    "for i in range(1,31):\n",
    "    date_list.append('2020-06-'+str(i))\n",
    "for i in range(1,32):\n",
    "    date_list.append('2020-07-'+str(i))\n",
    "for i in range(1,32):\n",
    "    date_list.append('2020-08-'+str(i))\n",
    "for i in range(1,31):\n",
    "    date_list.append('2020-09-'+str(i))\n",
    "for i in range(1,32):\n",
    "    date_list.append('2020-10-'+str(i))\n",
    "for i in range(1,31):\n",
    "    date_list.append('2020-11-'+str(i))\n",
    "for i in range(1,32):\n",
    "    date_list.append('2020-12-'+str(i))\n",
    "\n",
    "\n",
    "date_list_new = []\n",
    "\"\"\"\n",
    "for date_ in date:\n",
    "    date_start = date_list[date_list.index(date_)+6]\n",
    "    date_end = date_list[date_list.index(date_)+9]\n",
    "    date_list_new.append(date_start+' - ' + date_end)\n",
    "\"\"\"\n",
    "for date_ in date:\n",
    "    date_start = (date_+datetime.timedelta(days=6)).strftime(\"%Y-%m-%d\")\n",
    "    date_end = (date_+datetime.timedelta(days=9)).strftime(\"%Y-%m-%d\")\n",
    "    date_list_new.append(date_start+' - ' + date_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.DataFrame()                            # generate table\n",
    "out['ZIP'] = zipcode                            # zip code column\n",
    "out['date_start - date_end'] = date_list_new    # date column\n",
    "out['Predicted new cases'] = predict_upscale     # predicted new cases columns\n",
    "out['risk_score_level']=None\n",
    "###\n",
    "pop = pd.read_csv('LApopulation.csv', index_col = False)  # population data\n",
    "###\n",
    "for i in range(out.shape[0]):\n",
    "    for j in range(pop.shape[0]):\n",
    "        if (out.at[i,'ZIP'] == pop.at[j,'ZIP']):\n",
    "            out.at[i,'cases/population'] = 10000 * out.at[i,'Predicted new cases'] / pop.at[j,'population']\n",
    "\n",
    "print(out.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# defind the risk score level\n",
    "for i in range(len(out)):\n",
    "    if out['cases/population'][i]<=2:\n",
    "        out.loc[i,'risk_score_level']=1\n",
    "    elif out['cases/population'][i]<=10:\n",
    "        out.loc[i,'risk_score_level']=2\n",
    "    elif out['cases/population'][i]<=20:\n",
    "        out.loc[i,'risk_score_level']=3\n",
    "    else:\n",
    "        out.loc[i,'risk_score_level']=4\n",
    "print(out.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### change the output dataset name ###\n",
    "out.to_csv('LA_daily_out.csv')\n",
    "### ----------------------------   ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
